{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LiteLLM Proxy Connection Diagnosis\n",
        "\n",
        "This notebook diagnoses connection issues to the LiteLLM Proxy.\n",
        "\n",
        "It performs multiple steps to isolate why the API Key might be missing:\n",
        "1. **Direct HTTP Request**: Verifies network, API Key, and Endpoint validity using `requests`.\n",
        "2. **LiteLLM SDK Test (Standard)**: Standard `litellm.completion` call.\n",
        "3. **LiteLLM SDK Test (Manually Constructed URL)**: Investigating exact URL endpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading .env from: /Users/sacahan/Documents/workspace/TraitQuest/backend/.env\n",
            "PROXY URL: http://sacahan-ubunto:4000\n",
            "MODEL: gpt-4o\n",
            "API KEY: sk-A...v7Zw\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "from litellm import completion\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# 1. Load Environment Variables\n",
        "env_path = os.path.join(os.path.dirname(os.getcwd()), 'backend', '.env')\n",
        "print(f\"Loading .env from: {env_path}\")\n",
        "load_dotenv(env_path)\n",
        "\n",
        "API_KEY = \"sk-A_B3GLxZjx6IpAzUsFv7Zw\"\n",
        "API_BASE = \"http://sacahan-ubunto:4000\"\n",
        "MODEL = \"gpt-4o\"\n",
        "\n",
        "print(f\"PROXY URL: {API_BASE}\")\n",
        "print(f\"MODEL: {MODEL}\")\n",
        "print(f\"API KEY: {API_KEY[:4]}...{API_KEY[-4:] if API_KEY else 'None'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Direct connectivity test\n",
        "This tests if the server is reachable and valid at the `/chat/completions` endpoint.\n",
        "**Note:** We found the proxy listens at `/chat/completions`, NOT `/v1/chat/completions`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure base doesn't have trailing slash\n",
        "if API_BASE.endswith('/'):\n",
        "    base = API_BASE[:-1]\n",
        "else:\n",
        "    base = API_BASE\n",
        "\n",
        "# Construct explicit URL\n",
        "url = f\"{base}/chat/completions\"\n",
        "\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {API_KEY}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "data = {\n",
        "    \"model\": MODEL,\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Hello, direct test!\"}]\n",
        "}\n",
        "\n",
        "print(f\"Sending Direct Request to: {url}\")\n",
        "try:\n",
        "    resp = requests.post(url, json=data, headers=headers, timeout=10)\n",
        "    print(f\"Status Code: {resp.status_code}\")\n",
        "    if resp.status_code == 200:\n",
        "        print(\"‚úÖ Direct Connection Successful\")\n",
        "        print(\"Response:\", resp.json()['choices'][0]['message']['content'])\n",
        "    else:\n",
        "        print(\"‚ùå Direct Connection Failed\")\n",
        "        print(\"Body:\", resp.text)\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Exception during request: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: OpenAI SDK Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enable verbose logging\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url=API_BASE,  # Âü∫Á§é URLÔºå‰∏çÂê´ /chat/completions\n",
        "    api_key=API_KEY,\n",
        ")\n",
        "\n",
        "try:\n",
        "    response = client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        messages=[{\"content\": \"Hello from LiteLLM!\", \"role\": \"user\"}],\n",
        "    )\n",
        "    print(\"\\n‚úÖ OpenAI Success!\")\n",
        "    print(response.choices[0].message.content)\n",
        "except Exception as e:\n",
        "    print(\"\\n‚ùå OpenAI Failed:\")\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: LiteLLM SDK Test (Corrected URL)\n",
        "If the previous test failed with 'Request blocked', it implies incorrect endpoint construction (e.g., trying to hit root).\n",
        "We will try to force the path to `/chat/completions` by appending it to `api_base` IF we think LiteLLM strips it, OR verify path mechanics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92m15:43:38 - LiteLLM:DEBUG\u001b[0m: utils.py:475 - \n",
            "\n",
            "\u001b[92m15:43:38 - LiteLLM:DEBUG\u001b[0m: utils.py:475 - \u001b[92mRequest to litellm:\u001b[0m\n",
            "\u001b[92m15:43:38 - LiteLLM:DEBUG\u001b[0m: utils.py:475 - \u001b[92mlitellm.completion(model='gpt-4o', messages=[{'content': 'Hello from LiteLLM (Manual URL)!', 'role': 'user'}], base_url='http://sacahan-ubunto:4000', api_key='sk-A_B3GLxZjx6IpAzUsFv7Zw')\u001b[0m\n",
            "\u001b[92m15:43:38 - LiteLLM:DEBUG\u001b[0m: utils.py:475 - \n",
            "\n",
            "\u001b[92m15:43:38 - LiteLLM:DEBUG\u001b[0m: utils.py:999 - Removing thought signatures from tool call IDs for non-Gemini model\n",
            "\u001b[92m15:43:38 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:523 - self.optional_params: {}\n",
            "\u001b[92m15:43:38 - LiteLLM:DEBUG\u001b[0m: utils.py:475 - SYNC kwargs[caching]: False; litellm.cache: None; kwargs.get('cache')['no-cache']: False\n",
            "\u001b[92m15:43:38 - LiteLLM:INFO\u001b[0m: utils.py:3872 - \n",
            "LiteLLM completion() model= gpt-4o; provider = openai\n",
            "\u001b[92m15:43:38 - LiteLLM:DEBUG\u001b[0m: utils.py:3875 - \n",
            "LiteLLM: Params passed to completion() {'model': 'gpt-4o', 'functions': None, 'function_call': None, 'temperature': None, 'top_p': None, 'n': None, 'stream': None, 'stream_options': None, 'stop': None, 'max_tokens': None, 'max_completion_tokens': None, 'modalities': None, 'prediction': None, 'audio': None, 'presence_penalty': None, 'frequency_penalty': None, 'logit_bias': None, 'user': None, 'custom_llm_provider': 'openai', 'response_format': None, 'seed': None, 'tools': None, 'tool_choice': None, 'max_retries': None, 'logprobs': None, 'top_logprobs': None, 'extra_headers': None, 'api_version': None, 'parallel_tool_calls': None, 'drop_params': None, 'allowed_openai_params': None, 'reasoning_effort': None, 'verbosity': None, 'additional_drop_params': None, 'messages': [{'content': 'Hello from LiteLLM (Manual URL)!', 'role': 'user'}], 'thinking': None, 'web_search_options': None, 'safety_identifier': None, 'service_tier': None}\n",
            "\u001b[92m15:43:38 - LiteLLM:DEBUG\u001b[0m: utils.py:3878 - \n",
            "LiteLLM: Non-Default params passed to completion() {}\n",
            "\u001b[92m15:43:38 - LiteLLM:DEBUG\u001b[0m: utils.py:475 - Final returned optional params: {'extra_body': {}}\n",
            "\u001b[92m15:43:38 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:523 - self.optional_params: {'extra_body': {}}\n",
            "\u001b[92m15:43:38 - LiteLLM:DEBUG\u001b[0m: utils.py:5428 - checking potential_model_names in litellm.model_cost: {'split_model': 'gpt-4o', 'combined_model_name': 'openai/gpt-4o', 'stripped_model_name': 'gpt-4o', 'combined_stripped_model_name': 'openai/gpt-4o', 'custom_llm_provider': 'openai'}\n",
            "\u001b[92m15:43:38 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1072 - \u001b[92m\n",
            "\n",
            "POST Request Sent from LiteLLM:\n",
            "curl -X POST \\\n",
            "http://sacahan-ubunto:4000 \\\n",
            "-d '{'model': 'gpt-4o', 'messages': [{'content': 'Hello from LiteLLM (Manual URL)!', 'role': 'user'}], 'extra_body': {}}'\n",
            "\u001b[0m\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Experiment 5: Manually append /chat/completions to api_base ---\n",
            "Testing api_base='http://sacahan-ubunto:4000/chat/completions'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92m15:43:40 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1145 - RAW RESPONSE:\n",
            "{\"id\": \"chatcmpl-D3dSFVe2CcFBleD2sSKOOffAwwhn9\", \"choices\": [{\"finish_reason\": \"stop\", \"index\": 0, \"logprobs\": null, \"message\": {\"content\": \"Hello! \\ud83d\\udc4b It\\u2019s great to connect with you via LiteLLM (Manual URL). How can I assist you today? \\ud83d\\ude0a\", \"refusal\": null, \"role\": \"assistant\", \"annotations\": null, \"audio\": null, \"function_call\": null, \"tool_calls\": null, \"provider_specific_fields\": {\"padding\": \"abcdefghijklmn\"}}, \"provider_specific_fields\": {\"content_filter_results\": {\"hate\": {\"filtered\": false, \"severity\": \"safe\"}, \"self_harm\": {\"filtered\": false, \"severity\": \"safe\"}, \"sexual\": {\"filtered\": false, \"severity\": \"safe\"}, \"violence\": {\"filtered\": false, \"severity\": \"safe\"}}}}], \"created\": 1769759020, \"model\": \"github_copilot/gpt-4o-2024-11-20\", \"object\": \"chat.completion\", \"service_tier\": null, \"system_fingerprint\": \"fp_3eed281ddb\", \"usage\": {\"completion_tokens\": 28, \"prompt_tokens\": 16, \"total_tokens\": 44, \"completion_tokens_details\": {\"accepted_prediction_tokens\": 0, \"audio_tokens\": null, \"reasoning_tokens\": null, \"rejected_prediction_tokens\": 0}, \"prompt_tokens_details\": {\"audio_tokens\": null, \"cached_tokens\": 0}}, \"prompt_filter_results\": [{\"content_filter_results\": {\"hate\": {\"filtered\": false, \"severity\": \"safe\"}, \"self_harm\": {\"filtered\": false, \"severity\": \"safe\"}, \"sexual\": {\"filtered\": false, \"severity\": \"safe\"}, \"violence\": {\"filtered\": false, \"severity\": \"safe\"}}, \"prompt_index\": 0}]}\n",
            "\n",
            "\n",
            "\u001b[92m15:43:40 - LiteLLM:INFO\u001b[0m: utils.py:1621 - Wrapper: Completed Call, calling success_handler\n",
            "\u001b[92m15:43:40 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:964 - selected model name for cost calculation: github_copilot/gpt-4o-2024-11-20\n",
            "\u001b[92m15:43:40 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1837 - Logging Details LiteLLM-Success Call: Cache_hit=None\n",
            "\u001b[92m15:43:40 - LiteLLM:DEBUG\u001b[0m: utils.py:5428 - checking potential_model_names in litellm.model_cost: {'split_model': 'github_copilot/gpt-4o-2024-11-20', 'combined_model_name': 'openai/github_copilot/gpt-4o-2024-11-20', 'stripped_model_name': 'github_copilot/gpt-4o-2024-11-20', 'combined_stripped_model_name': 'openai/github_copilot/gpt-4o-2024-11-20', 'custom_llm_provider': 'openai'}\n",
            "\u001b[92m15:43:40 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:964 - selected model name for cost calculation: github_copilot/gpt-4o-2024-11-20\n",
            "\u001b[92m15:43:40 - LiteLLM:DEBUG\u001b[0m: utils.py:5698 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n",
            "\u001b[92m15:43:40 - LiteLLM:DEBUG\u001b[0m: utils.py:5428 - checking potential_model_names in litellm.model_cost: {'split_model': 'github_copilot/gpt-4o-2024-11-20', 'combined_model_name': 'openai/github_copilot/gpt-4o-2024-11-20', 'stripped_model_name': 'github_copilot/gpt-4o-2024-11-20', 'combined_stripped_model_name': 'openai/github_copilot/gpt-4o-2024-11-20', 'custom_llm_provider': 'openai'}\n",
            "\u001b[92m15:43:40 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:1378 - litellm.cost_calculator.py::completion_cost() - Error calculating cost for model=github_copilot/gpt-4o-2024-11-20 - This model isn't mapped yet. model=github_copilot/gpt-4o-2024-11-20, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.\n",
            "\u001b[92m15:43:40 - LiteLLM:DEBUG\u001b[0m: utils.py:5698 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n",
            "\u001b[92m15:43:40 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:1378 - litellm.cost_calculator.py::completion_cost() - Error calculating cost for model=github_copilot/gpt-4o-2024-11-20 - This model isn't mapped yet. model=github_copilot/gpt-4o-2024-11-20, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.\n",
            "\u001b[92m15:43:40 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:964 - selected model name for cost calculation: github_copilot/gpt-4o-2024-11-20\n",
            "\u001b[92m15:43:40 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:964 - selected model name for cost calculation: github_copilot/gpt-4o-2024-11-20\n",
            "\u001b[92m15:43:40 - LiteLLM:DEBUG\u001b[0m: utils.py:5428 - checking potential_model_names in litellm.model_cost: {'split_model': 'github_copilot/gpt-4o-2024-11-20', 'combined_model_name': 'openai/github_copilot/gpt-4o-2024-11-20', 'stripped_model_name': 'github_copilot/gpt-4o-2024-11-20', 'combined_stripped_model_name': 'openai/github_copilot/gpt-4o-2024-11-20', 'custom_llm_provider': 'openai'}\n",
            "\u001b[92m15:43:40 - LiteLLM:DEBUG\u001b[0m: utils.py:5428 - checking potential_model_names in litellm.model_cost: {'split_model': 'github_copilot/gpt-4o-2024-11-20', 'combined_model_name': 'openai/github_copilot/gpt-4o-2024-11-20', 'stripped_model_name': 'github_copilot/gpt-4o-2024-11-20', 'combined_stripped_model_name': 'openai/github_copilot/gpt-4o-2024-11-20', 'custom_llm_provider': 'openai'}\n",
            "\u001b[92m15:43:40 - LiteLLM:DEBUG\u001b[0m: utils.py:5698 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n",
            "\u001b[92m15:43:40 - LiteLLM:DEBUG\u001b[0m: utils.py:5698 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n",
            "\u001b[92m15:43:40 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:1378 - litellm.cost_calculator.py::completion_cost() - Error calculating cost for model=github_copilot/gpt-4o-2024-11-20 - This model isn't mapped yet. model=github_copilot/gpt-4o-2024-11-20, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.\n",
            "\u001b[92m15:43:40 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:1378 - litellm.cost_calculator.py::completion_cost() - Error calculating cost for model=github_copilot/gpt-4o-2024-11-20 - This model isn't mapped yet. model=github_copilot/gpt-4o-2024-11-20, custom_llm_provider=openai. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json.\n",
            "\u001b[92m15:43:40 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:964 - selected model name for cost calculation: gpt-4o\n",
            "\u001b[92m15:43:40 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:964 - selected model name for cost calculation: gpt-4o\n",
            "\u001b[92m15:43:40 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:763 - No margin config found. Provider: openai, Available configs: []\n",
            "\u001b[92m15:43:40 - LiteLLM:DEBUG\u001b[0m: cost_calculator.py:763 - No margin config found. Provider: openai, Available configs: []\n",
            "\u001b[92m15:43:40 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1440 - response_cost: 0.00032\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Experiment 5 Success!\n",
            "Hello! üëã It‚Äôs great to connect with you via LiteLLM (Manual URL). How can I assist you today? üòä\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[92m15:43:40 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1440 - response_cost: 0.00032\n",
            "\u001b[92m15:43:40 - LiteLLM:DEBUG\u001b[0m: utils.py:5428 - checking potential_model_names in litellm.model_cost: {'split_model': 'github_copilot/gpt-4o-2024-11-20', 'combined_model_name': 'openai/github_copilot/gpt-4o-2024-11-20', 'stripped_model_name': 'github_copilot/gpt-4o-2024-11-20', 'combined_stripped_model_name': 'openai/github_copilot/gpt-4o-2024-11-20', 'custom_llm_provider': 'openai'}\n",
            "\u001b[92m15:43:40 - LiteLLM:DEBUG\u001b[0m: utils.py:5698 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n",
            "\u001b[92m15:43:40 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:4574 - Model=github_copilot/gpt-4o-2024-11-20 is not mapped in model cost map. Defaulting to None model_cost_information for standard_logging_payload\n",
            "\u001b[92m15:43:40 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:1874 - Logging Details LiteLLM-Success Call streaming complete\n",
            "\u001b[92m15:43:40 - LiteLLM:DEBUG\u001b[0m: utils.py:5428 - checking potential_model_names in litellm.model_cost: {'split_model': 'github_copilot/gpt-4o-2024-11-20', 'combined_model_name': 'openai/github_copilot/gpt-4o-2024-11-20', 'stripped_model_name': 'github_copilot/gpt-4o-2024-11-20', 'combined_stripped_model_name': 'openai/github_copilot/gpt-4o-2024-11-20', 'custom_llm_provider': 'openai'}\n",
            "\u001b[92m15:43:40 - LiteLLM:DEBUG\u001b[0m: utils.py:5698 - Error getting model info: This model isn't mapped yet. Add it here - https://github.com/BerriAI/litellm/blob/main/model_prices_and_context_window.json\n",
            "\u001b[92m15:43:40 - LiteLLM:DEBUG\u001b[0m: litellm_logging.py:4574 - Model=github_copilot/gpt-4o-2024-11-20 is not mapped in model cost map. Defaulting to None model_cost_information for standard_logging_payload\n"
          ]
        }
      ],
      "source": [
        "# Enable verbose logging\n",
        "import litellm\n",
        "litellm._turn_on_debug()\n",
        "\n",
        "print(\"\\n--- Experiment 5: Manually append /chat/completions to api_base ---\")\n",
        "# Normally 'openai' provider appends /chat/completions. But if API_BASE is treated as the full URL, we might need to specify exact path.\n",
        "# However, if we append it, does LiteLLM append it again? Let's check logs.\n",
        "manual_url = f\"{API_BASE}/chat/completions\"\n",
        "print(f\"Testing api_base='{manual_url}'\")\n",
        "try:\n",
        "    response = litellm.completion(\n",
        "        model=MODEL,\n",
        "        messages=[{\"content\": \"Hello from LiteLLM (Manual URL)!\", \"role\": \"user\"}],\n",
        "        base_url=API_BASE,\n",
        "        api_key=API_KEY,\n",
        "        # custom_llm_provider=\"openai\",\n",
        "        # extra_headers={\n",
        "        #     \"Content-Type\": \"application/json\",\n",
        "        #     \"Authorization\": f\"Bearer {API_KEY}\",\n",
        "        # },\n",
        "    )\n",
        "    print(\"‚úÖ Experiment 5 Success!\")\n",
        "    print(response.choices[0].message.content)\n",
        "except Exception as e:\n",
        "    print(\"‚ùå Experiment 5 Failed:\")\n",
        "    print(e)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
