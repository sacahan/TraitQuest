{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LiteLLM Proxy Connection Diagnosis\n",
        "\n",
        "This notebook diagnoses connection issues to the LiteLLM Proxy.\n",
        "\n",
        "It performs multiple steps to isolate why the API Key might be missing:\n",
        "1. **Direct HTTP Request**: Verifies network, API Key, and Endpoint validity using `requests`.\n",
        "2. **LiteLLM SDK Test (Standard)**: Standard `litellm.completion` call.\n",
        "3. **LiteLLM SDK Test (Manually Constructed URL)**: Investigating exact URL endpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "from litellm import completion\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# 1. Load Environment Variables\n",
        "env_path = os.path.join(os.path.dirname(os.getcwd()), 'backend', '.env')\n",
        "print(f\"Loading .env from: {env_path}\")\n",
        "load_dotenv(env_path)\n",
        "\n",
        "API_KEY = os.environ(\"LITELLM_PROXY_API_KEY\")\n",
        "API_BASE = \"http://sacahan-ubunto:4000\"\n",
        "MODEL = \"gpt-4o\"\n",
        "\n",
        "print(f\"PROXY URL: {API_BASE}\")\n",
        "print(f\"MODEL: {MODEL}\")\n",
        "print(f\"API KEY: {API_KEY[:4]}...{API_KEY[-4:] if API_KEY else 'None'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Direct connectivity test\n",
        "This tests if the server is reachable and valid at the `/chat/completions` endpoint.\n",
        "**Note:** We found the proxy listens at `/chat/completions`, NOT `/v1/chat/completions`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure base doesn't have trailing slash\n",
        "if API_BASE.endswith('/'):\n",
        "    base = API_BASE[:-1]\n",
        "else:\n",
        "    base = API_BASE\n",
        "\n",
        "# Construct explicit URL\n",
        "url = f\"{base}/chat/completions\"\n",
        "\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {API_KEY}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "data = {\n",
        "    \"model\": MODEL,\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Hello, direct test!\"}]\n",
        "}\n",
        "\n",
        "print(f\"Sending Direct Request to: {url}\")\n",
        "try:\n",
        "    resp = requests.post(url, json=data, headers=headers, timeout=10)\n",
        "    print(f\"Status Code: {resp.status_code}\")\n",
        "    if resp.status_code == 200:\n",
        "        print(\"✅ Direct Connection Successful\")\n",
        "        print(\"Response:\", resp.json()['choices'][0]['message']['content'])\n",
        "    else:\n",
        "        print(\"❌ Direct Connection Failed\")\n",
        "        print(\"Body:\", resp.text)\n",
        "except Exception as e:\n",
        "    print(f\"❌ Exception during request: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: OpenAI SDK Test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enable verbose logging\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url=API_BASE,  # 基礎 URL，不含 /chat/completions\n",
        "    api_key=API_KEY,\n",
        ")\n",
        "\n",
        "try:\n",
        "    response = client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        messages=[{\"content\": \"Hello from LiteLLM!\", \"role\": \"user\"}],\n",
        "    )\n",
        "    print(\"\\n✅ OpenAI Success!\")\n",
        "    print(response.choices[0].message.content)\n",
        "except Exception as e:\n",
        "    print(\"\\n❌ OpenAI Failed:\")\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: LiteLLM SDK Test (Corrected URL)\n",
        "If the previous test failed with 'Request blocked', it implies incorrect endpoint construction (e.g., trying to hit root).\n",
        "We will try to force the path to `/chat/completions` by appending it to `api_base` IF we think LiteLLM strips it, OR verify path mechanics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enable verbose logging\n",
        "import litellm\n",
        "litellm._turn_on_debug()\n",
        "\n",
        "print(\"\\n--- Experiment 5: Manually append /chat/completions to api_base ---\")\n",
        "# Normally 'openai' provider appends /chat/completions. But if API_BASE is treated as the full URL, we might need to specify exact path.\n",
        "# However, if we append it, does LiteLLM append it again? Let's check logs.\n",
        "manual_url = f\"{API_BASE}/chat/completions\"\n",
        "print(f\"Testing api_base='{manual_url}'\")\n",
        "try:\n",
        "    response = completion(\n",
        "        model=MODEL,\n",
        "        messages=[{\"content\": \"Hello from LiteLLM (Manual URL)!\", \"role\": \"user\"}],\n",
        "        base_url=API_BASE,\n",
        "        api_key=API_KEY,\n",
        "        # custom_llm_provider=\"openai\",\n",
        "        # extra_headers={\n",
        "        #     \"Content-Type\": \"application/json\",\n",
        "        #     \"Authorization\": f\"Bearer {API_KEY}\",\n",
        "        # },\n",
        "    )\n",
        "    print(\"✅ Experiment 5 Success!\")\n",
        "    print(response.choices[0].message.content)\n",
        "except Exception as e:\n",
        "    print(\"❌ Experiment 5 Failed:\")\n",
        "    print(e)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
